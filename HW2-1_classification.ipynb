{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlKkVpnSYuMbMgignHAVt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaoshangqichuang/MLnotes/blob/main/HW2_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O7C1TiWZonY"
      },
      "source": [
        "# Logistic回归"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PRPVkmhMXjW"
      },
      "source": [
        "## 数据预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrAc5l1GZuzg"
      },
      "source": [
        "### 下载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww4-VJoJqE-_"
      },
      "source": [
        "!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMZoD2LvZyZf"
      },
      "source": [
        "### 数据标准化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi8_NvIAQmUB"
      },
      "source": [
        "#数据标准化\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "X_train_fpath = './timit_11/train_11.npy'\n",
        "Y_train_fpath = './timit_11/train_label_11.npy'\n",
        "X_test_fpath = './timit_11/test_11.npy'\n",
        "'''\n",
        "with open(X_train_fpath) as f:\n",
        "  next(f)\n",
        "  X_train = np.array([line.strip('\\n').split(',')[1:] for line in f],dtype = float)\n",
        "with open(Y_train_fpath) as f:\n",
        "  next(f)\n",
        "  Y_train = np.array([line.strip('\\n').split(',')[1:] for line in f],dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "  next(f)\n",
        "  X_test = np.array([line.strip('\\n').split(',')[1:] for line in f],dtype = float)\n",
        "'''\n",
        "X_train = np.load(X_train_fpath)\n",
        "Y_train = np.load(Y_train_fpath)\n",
        "X_test = np.load(X_test_fpath)\n",
        "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
        "  if specified_column == None:\n",
        "    specified_column = np.arange(X.shape[1])\n",
        "  if train:\n",
        "    X_mean = np.mean(X[:,specified_column],0).reshape(1,-1)\n",
        "    X_std = np.std(X[:,specified_column],0).reshape(1,-1)\n",
        "  X[:,specified_column] = (X[:,specified_column]-X_mean) / (X_std + 1e-8) \n",
        "  return X,X_mean, X_std\n",
        "\n",
        "def _train_dev_split(X,Y,dev_ratio=0.25):\n",
        "  #将训练集和验证集分开\n",
        "  train_size = int(len(X)*(1-dev_ratio))\n",
        "  return X[:train_size],Y[:train_size],X[train_size:],Y[train_size:]\n",
        "dev_ratio = 0.1\n",
        "X_train,Y_train,X_dev,Y_dev = _train_dev_split(X_train,Y_train,dev_ratio = dev_ratio)\n",
        "train_size = X_train.shape[0]\n",
        "dev_size = X_dev.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "data_dim = X_train.shape[1]\n",
        "print('Size of training set:{}'.format(train_size))\n",
        "print('Size of development set:{}'.format(dev_size))\n",
        "print('Size of test set:{}'.format(test_size))\n",
        "print('Dimension of data:{}'.format(data_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nvQwLVlA_kY"
      },
      "source": [
        "### 构建数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQGnFEUrY4d1"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "class TIMIDataset(Dataset):\n",
        "  def __init__(self,X,y=None):\n",
        "    self.data = torch.from_numpy(X).float()\n",
        "    if y is not None:\n",
        "      y = y.astype(np.int)\n",
        "      self.label = torch.LongTensor(y)\n",
        "    else:\n",
        "      self.label = None\n",
        "  def __getitem__(self,idx):\n",
        "    if self.label is not None:\n",
        "      return self.data[idx],self.label[idx]\n",
        "    else:\n",
        "      return self.data[idx]\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ghlIbC8t8_"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_set = TIMIDataset(X_train,Y_train)\n",
        "val_set = TIMIDataset(X_dev,Y_dev)\n",
        "test_set = TIMIDataset(X_test)\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "len(set(Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbJA4yHPWC6D"
      },
      "source": [
        "import gc\n",
        "del X_train, Y_train, X_dev, Y_dev\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-v1LZDBFCV"
      },
      "source": [
        "## 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEVSjzTABIje"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier,self).__init__()\n",
        "    self.layer1 = nn.Linear(429,1024)\n",
        "    self.layer2 = nn.Linear(1024,512)\n",
        "    self.layer3 = nn.Linear(512,128)\n",
        "    self.out = nn.Linear(128,39)\n",
        "    self.act_fn = nn.Sigmoid()\n",
        "  def forward(self,x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.act_fn(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.act_fn(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.act_fn(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "get_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlB1UjPGMQ8I"
      },
      "source": [
        "### 固定随机种子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uqYdlkYBrV0"
      },
      "source": [
        "def same_seeds(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcslKYK0DzUp"
      },
      "source": [
        "same_seeds(0)\n",
        "device = get_device()\n",
        "num_epoch = 20\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model_path = './model.ckpt'\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yevTfqrrMiY-"
      },
      "source": [
        "### 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5bDpYuoF0LA"
      },
      "source": [
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "  train_acc = 0.0\n",
        "  train_loss = 0.0\n",
        "  val_acc = 0.0\n",
        "  val_loss = 0.0\n",
        "  model.train()\n",
        "  for i,data in enumerate(train_loader):\n",
        "    inputs,labels = data\n",
        "    inputs,labels = inputs.to(device),labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    batch_loss = criterion(outputs,labels.long())\n",
        "    _,train_pred = torch.max(outputs,1)\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (train_pred.cpu()==labels.cpu()).sum().item()\n",
        "    train_loss += batch_loss.item()\n",
        "  if len(val_set)>0:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i,data in enumerate(val_loader):\n",
        "        inputs,labels = data\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        batch_loss = criterion(outputs,labels)\n",
        "        _,val_pred = torch.max(outputs,1)\n",
        "        val_acc += (val_pred.cpu()==labels.cpu()).sum().item()\n",
        "        val_loss += batch_loss.item()\n",
        "      print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "      if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(),model_path)\n",
        "        print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "  else:\n",
        "     print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMl-YY_vMmAG"
      },
      "source": [
        "## 预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0CCQ0NMpro"
      },
      "source": [
        "test_set = TIMITDataset(X_test,None)\n",
        "test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYOkdB12NIRZ"
      },
      "source": [
        "predict = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i,data in enumerate(test_loader):\n",
        "    inputs = data\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = model(inputs)\n",
        "    _,test_pred = torch.max(outputs,1)\n",
        "    for y in test_pred.cpu().numpy():\n",
        "      predict.append(y)\n",
        "with open('prediction.csv','w') as f:\n",
        "  f.write('Id,Class\\n')\n",
        "  for i,y in enumerate(predict):\n",
        "    f.write('{},{}\\n'.format(i,y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
