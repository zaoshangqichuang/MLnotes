# 分类问题classification
## 二分类问题
直接用regression来代替classification的时候，为了减少误差可能得到的不会是最优的分类。
### perception
一般输出值为一个向量，利用**soft-max**将向量标准化到(0,1)  
yi = exp(y1)/∑exp(yi)  
* 两个优点
1. 使得概率值标准化到(0,1)
2. 使得最大值和最小值的差距更大  

**朴素贝叶斯算法**  
* 基本假设：每个样本都是相互独立的，不相互影响。  
* 具体步骤:
1. 假设样本每个class都服从一定的分布，从各个class中sample一些样本  
2. 根据样本拟合为高斯分布(只有两个类拟合为二项分布)，计算分布相关的参数(采用最大似然估计，损失函数等于交叉熵函数)  
3. 将其他样本代入分布中计算概率，概率最大的为所属的类  
*注：再进行流计算时，一般估算batch中mu和thegema的方法是通过移动平均值*  
### Logistic Regression
* 和回归的区别和共同点
1. 输出还需要使用激活函数
2. 损失函数:最大似然估计-交叉熵函数，回归使用RMSE
3. 参数更新的公式是一模一样的 
 
## discriminative vs generative
* 用同一个function set 但是做的是不同的假设，因此得到的结果还是不同的  
* discriminative的表现往往比generative的结果好  
* training data的样本量需求比较少  
* generative model 的鲁棒性更强  
* 在语音识别中，其实是discriminative和generative的组合，将先验和条件概率拆开  

## LR的限制
当样本量和特征有限时，分割线不能完成有效的划分
解决方法：  
* 特征转换(feature_transformation)
* 将多个LR叠加起来，其实就是**deep learning**
